\chapter{State of the Art}\label{chap:art}

\section{Introduction}
Regular expressions (regex) remain one of the most powerful and widely adopted tools for string pattern matching across programming languages, search tools, and data processing pipelines. While their theoretical foundation lies in formal language theory, the practical implementation of regex engines often diverges from the idealized models. This chapter mentions the state of the art in regex engine designs and regex language features, with a particular focus on performance trade-offs, security concerns, and evolving capabilities.

\section{Regex Syntax Variants}
\label{sec:regex_syntax}

Beyond the theoretical framework of regular expressions lies a diverse set of practical syntax variants. Different tools, programming languages, and libraries adopt different interpretations and extensions of regex syntax, leading to a rich but inconsistent ecosystem. Two of the most influential and widely adopted syntax families are POSIX and PCRE.

%\subsection{POSIX Regular Expressions}

%The POSIX (Portable Operating System Interface) standard defines a family of regular expression syntaxes intended to promote consistency across UNIX-like systems. POSIX defines two primary levels of syntax: Basic Regular Expressions (BRE) and Extended Regular Expressions (ERE). These are used in many command-line utilities such as \texttt{grep} \cite{sw_grep}, \texttt{sed} \cite{sw_sed}, and \texttt{awk} \cite{sw_gawk}.
%
%In BRE, several metacharacters such as parentheses, braces, and the plus sign require escaping to be interpreted as operators. ERE, in contrast, relaxes this requirement and provides a more convenient syntax, allowing characters like \texttt{+}, \texttt{?}, \texttt{|}, and parentheses to act as operators without escaping. \cite{regex_opengroup}
%
%A defining feature of POSIX regular expressions is their leftmost-longest match semantics. When multiple matches are possible, the engine must return the one that starts at the earliest position in the input and is the longest among those starting at that position. This requirement ensures deterministic behavior but can increase the complexity of the matching algorithm. \cite{regex_opengroup}
%
%POSIX regular expressions do not support many advanced features common in other regex dialects. For example, they lack support for backreferences, lookahead/lookbehind assertions, non-greedy quantifiers, and conditional expressions. This makes POSIX regexes more limited in expressiveness but often easier to reason about and implement efficiently.

\subsection{PCRE: Perl-Compatible Regular Expressions}

PCRE (Perl-Compatible Regular Expressions) define an expressive and flexible regex syntax, modeled after the pattern language used in the Perl programming language. It has become the standard for regular expression syntax in many modern programming environments, including PHP, Python, JavaScript, and others.

PCRE introduces a wide range of features that go far beyond the capabilities of POSIX regexes. These include:

\begin{itemize}
	\item \textbf{Backreferences}, allowing a pattern to refer to previously matched groups.\\
	Example: \texttt{(a)b\textbackslash1} matches \texttt{aba} (the \texttt{\textbackslash1} refers to the first captured \texttt{a}).
	
	\item \textbf{Lookahead and lookbehind assertions}, enabling matches based on surrounding context without consuming it.\\
	Examples:
	\begin{itemize}
		\item Lookahead: \texttt{foo(?=bar)} matches \texttt{foo} only if followed by \texttt{bar}.
		\item Lookbehind: \texttt{(?<=foo)bar} matches \texttt{bar} only if preceded by \texttt{foo}.
	\end{itemize}
	
	\item \textbf{Non-greedy quantifiers}, which match as little as possible.\\
	Example: \texttt{<.*?>} matches the shortest HTML-like tag, such as \texttt{<b>} in \texttt{<b>bold</b>}, instead of greedily matching the entire string.
	
	\item \textbf{Named capture groups}, for improved readability and maintainability.\\
	Example: \texttt{(?<year>\textbackslash d\{4\})-(?<month>\textbackslash d\{2\})} captures \texttt{2025-09} with named groups \texttt{year} and \texttt{month}.
	
	\item \textbf{Conditional expressions}, recursion, atomic groups, and other advanced constructs.\\
	Examples:
	\begin{itemize}
		\item Conditional: \texttt{(a)?b(?(1)c|d)} matches \texttt{abc} if \texttt{a} is present, \texttt{bd} otherwise.
		\item Recursion: \texttt{(a|b(?1))*} matches balanced nested patterns using self-reference.
		\item Atomic group: \texttt{(?>a+)} prevents backtracking inside the group.
	\end{itemize}
\end{itemize}

The added power of PCRE comes at the cost of increased complexity in both the syntax and the engine. To support these features, most PCRE-compatible engines rely on backtracking-based evaluation, which explores all potential matching paths through a pattern. While this approach enables support for advanced constructs, it also introduces the risk of exponential-time behavior in certain patterns—especially those involving nested quantifiers or ambiguous alternations.

\subsection{POSIX Regular Expressions}

The POSIX (Portable Operating System Interface) standard defines a family of regular expression syntaxes intended to promote consistency across UNIX-like systems. POSIX defines two primary levels of syntax: \textit{Basic Regular Expressions} (BRE) and \textit{Extended Regular Expressions} (ERE). These are used in many command-line utilities such as \texttt{grep} \cite{sw_grep}, \texttt{sed} \cite{sw_sed}, and \texttt{awk} \cite{sw_gawk}.

In BRE, several metacharacters (such as parentheses, curly braces, and the plus sign) must be escaped to be interpreted as operators. For example:
\begin{itemize}
	\item \texttt{a\textbackslash\{3\textbackslash\}} matches exactly three consecutive \texttt{a}'s (\texttt{aaa}, for instance).
	\item \texttt{\textbackslash(a\textbar b\textbackslash)} matches either \texttt{a} or \texttt{b} using grouping and alternation.
\end{itemize}

In contrast, ERE relaxes these constraints and allows a more expressive and readable syntax without requiring escapes for most metacharacters. For example:
\begin{itemize}
	\item \texttt{a\{3\}} matches \texttt{aaa} (three consecutive \texttt{a}'s).
	\item \texttt{a+} matches one or more \texttt{a}'s.
	\item \texttt{a|b} matches either \texttt{a} or \texttt{b}.
	\item \texttt{(abc)?} matches either \texttt{abc} or $\varepsilon$.
\end{itemize}

A defining feature of POSIX regular expressions is their \textit{leftmost-longest match} semantics. When multiple matches are possible, the engine must return the one that starts at the earliest position in the input and is the longest among those starting at that position. This requirement ensures deterministic behavior but can increase the complexity of the matching algorithm. \cite{regex_opengroup}

%As seen, the POSIX standard does not support many advanced features common in other regex dialects. For instance, they lack support for backreferences. When these are introduced, the resulting languages are non-regular, meaning that they cannot be recognized by finite automata.

POSIX regular expressions do not support many advanced features common in other regex dialects. For example, they lack support for backreferences, lookahead/lookbehind assertions, non-greedy quantifiers, and conditional expressions. This makes POSIX regexes more limited in expressiveness but often easier to reason about and implement efficiently.

\section{Engine Architectures}
%Regex engines are typically implemented using one of the following architectures:
Regular expressions are most commonly used to look for patterns of strings, known as matching. The basis for this operation are the regular expression engines.
At a high level, regex engines can be grouped by how they traverse the implicit nondeterministic automaton of a pattern.
%Backtracking engines explore one path at a time, pushing alternative choices on a stack; NFA simulation engines keep all active states in parallel; DFA and hybrid engines trade memory for predictable time; and high-throughput engines emphasize vectorization and streaming.

\subsection{Backtracking NFA}
The classic backtracking NFA matcher is driven by the pattern: the regular expression acts like a small procedural program that dictates how the engine explores matches and handles failure. The engine begins at the start of the text and attempts to match the pattern from that position; if it fails, it “bumps along,” advancing one character and trying again. Once the first tokens of the pattern match the text, the engine proceeds through the regex. At any choice point—such as an alternation, an optional, or a quantifier—it selects one alternative to try and records the others, together with the current input position. If the chosen path later fails, the engine backtracks to the most recent choice point and resumes with a saved alternative. If all saved alternatives are exhausted, the attempt from that starting position fails and the engine bumps along to the next character.

If the engine reaches the end of the pattern with all constraints satisfied, it declares success and discards any remaining, unexplored alternatives. A key consequence is that the order of alternatives matters: typical backtracking engines implement leftmost-first semantics rather than leftmost-longest, so the first workable alternative can win even if a longer match exists.

Because the engine literally follows the structure of the regex, you control its search by how you write the pattern: place safer or more selective alternatives first, avoid ambiguous constructs under repetition, and structure patterns to minimize backtracking and to fail fast when appropriate.

Despite the name, a backtracking NFA engine is not an NFA in the formal sense used in automata theory. Theoretical NFAs are memoryless machines that recognize only regular languages, whereas real-world backtracking engines—such as those used in PCRE, Java, and Python—go beyond regularity (Câmpeanu et al. proved this using a pumping lemma in \cite{campeanu_reg_ereg}). They simulate nondeterministic behavior by exploring multiple paths through the pattern, but they do so using a stack and internal memory to track backtracking points, group captures, and even previous input matches. This allows them to support powerful features like backreferences and conditional expressions, which cannot be recognized by finite automata. The term “NFA” here refers to the engine’s matching strategy, inspired by the branching behavior of NFAs, rather than to its computational limitations.


\subsection{POSIX NFA}
Similarly to the classic NFA engine, the POSIX NFA engine will match in the same way but memorize and continue when a successful match is found. This is done to see if a longer, leftmost match can be found later on.

\subsection{DFA}
A DFA-based matcher is driven by the input text rather than the structure of the regular expression. The pattern is first compiled into a deterministic finite automaton (DFA), which enables the engine to scan the input in a single pass. As each character is read, the engine transitions deterministically from one state to another, maintaining only a single active state at any point during execution.

Each DFA state represents a set of possible continuations in the pattern, allowing the matcher to recognize valid strings without the need for backtracking. This results in highly predictable performance, with matching taking linear time in the length of the input.

In search mode, DFA-based engines typically return the leftmost match. Many implementations are also designed to return the leftmost-longest match by continuing the scan while remembering the last encountered accepting state. Because the DFA encodes all matching possibilities in advance, the matching process is entirely deterministic and independent of the syntactic order of alternatives in the pattern.

One practical consideration in DFA-based matching is the potential growth of the automaton during the compilation phase. In the worst case, the number of DFA states can grow exponentially with respect to the size of the regular expression, particularly for patterns involving complex nesting or alternation. While this does not affect runtime performance during matching — which remains linear — the memory and time costs of building the DFA can be significant. To address this, many modern implementations adopt strategies such as lazy DFA construction or hybrid evaluation models that balance determinism with scalability.


%A DFA-based matcher is driven by the input text rather than by the pattern. After compiling the pattern into a deterministic automaton, the engine scans the input once, advancing a single current state per character with no need to branch or revisit earlier positions. Intuitively, each DFA state encodes all viable continuations of the match; this removes the need for backtracking and yields predictable, linear-time behavior in the length of the input. In search mode, DFA-style engines are typically implemented to report the leftmost match, and many implementations further return the leftmost-longest match by continuing to advance while remembering the last accepting position. Because the traversal is deterministic, there is no notion of “trying one alternative before another”: the match policy is fixed by the automaton, not by the syntactic order of subexpressions.
%
%This approach is efficient, but it comes with trade-offs. First, you cannot influence the exploration order to emulate backtracking behaviors; the engine will not “prefer” one branch over another, and constructs like atomic groups or possessive quantifiers are largely moot because there is no backtracking to prune. Second, features that go beyond regular languages—most notably backreferences—are not supported in a pure DFA framework, and general look-around assertions are often unavailable or only supported in restricted, regular cases. Third, while matching is fast, precompilation can be more expensive in time and memory due to determinization; in the worst case, the number of DFA states may grow exponentially with the size of the expression, which practical systems mitigate with lazy or hybrid techniques. Finally, although some DFA-centric libraries provide submatch extraction using taged automata or similar mechanisms, rich capturing semantics are more limited than in backtracking engines.

\subsection{Hybrid}
Hybrid engines try to take the best of both worlds in both NFAs and DFAs. They perform a depth-first search through the space of matches. At each point of nondeterminism—due to alternation, optional constructs, or unbounded quantifiers — they choose one option to continue and save the others as choice points on an internal stack. If a later step fails, the engine pops a choice point and resumes from there. This approach yields intuitive behavior and supports rich features, including capturing groups, backreferences, and look-around. However, in the presence of ambiguous subpatterns under repetition, the number of explored paths can grow exponentially, making these engines particularly susceptible to ReDoS.

Spencer’s classic backtracking implementation is perhaps the best known and most used, embodying closely the architecture described above. Conceptually, its state comprises the current regex node, the current input index, and a stack of saved alternatives. Consider the pattern \texttt{(a|aa)+\$} against the input \texttt{aaaa\ldots a} without a trailing \texttt{b}. The matcher repeatedly chooses the left alternative \texttt{a}, consuming a single character while saving a choice point for the \texttt{aa} branch at each iteration. At the end of input it fails to satisfy the end anchor, so it begins to backtrack, popping choice points and trying to repartition the run of \texttt{a}s using \texttt{aa} segments. The number of such partitions grows exponentially with the length of the input, and the engine must examine many of them before determining there is no match. This example illustrates how overlapping alternatives inside a quantifier, combined with a terminal failure, can lead directly to catastrophic backtracking.
%Great examples of this engine being used widely are the tools GNU egrep and awk.
The tools GNU egrep and awk use a hybrid approach for this. They choose

\section{Engines and Libraries}
Modern regular expression engines vary widely in their design goals, trade-offs, and supported features. While traditional engines prioritize expressiveness (often at the cost of performance guarantees) there has been a growing interest in implementations that prioritize safety, predictability, and efficiency, especially in systems that process untrusted or large-scale input. This section presents a selection of such libraries and engines.


\subsection{RE2}
RE2 is a regular expression engine developed by Russ Cox at Google, designed to provide predictable performance and strong safety guarantees. Unlike backtracking engines, RE2 avoids features that lead to exponential-time behavior, such as backreferences and arbitrary lookbehinds. By restricting itself to regular languages, RE2 ensures that all matches can be performed in linear time. \cite{russ_cox_regexp3}

The engine compiles patterns into automata using a combination of DFA and NFA techniques. For simple match queries, RE2 prefers deterministic finite automata (DFA) for their speed and predictability. When submatch extraction is needed, it may fall back to an NFA simulation, provided the regex satisfies certain properties (e.g., being one-pass). \cite{russ_cox_regexp3}

By trading off advanced features like backreferences for performance and safety, RE2 provides a robust alternative to PCRE-style engines, especially in environments where worst-case behavior must be avoided.

RE2 can also operate in either POSIX mode (accepting POSIX syntax regular expressions) or Perl mode (accepting PCRE syntax regular expressions). \cite{russ_cox_re2}
%https://swtch.com/~rsc/regexp/regexp1.html

\subsection{PCRE2}
PCRE2 is a modern and widely adopted regular expression library that implements the rich, feature-complete syntax mentioned above (PCRE). \cite{sw_pcre2}

At its core, PCRE2 relies on a backtracking-based matcher, which simulates nondeterminism by exploring alternative execution paths through the pattern. Although this resembles the behavior of nondeterministic finite automata (NFAs), it does not correspond to an NFA in the theoretical sense. Instead, the engine maintains an explicit control stack and memory for backtracking, enabling it to handle features like backreferences and complex group captures. This matching strategy enables high expressiveness but comes with the risk of exponential time complexity in certain pathological cases, particularly when ambiguous repetition and nested alternations are involved. \cite{sw_pcre2}

PCRE2 also offers an alternative, limited matching mode based on deterministic finite automata (DFA), which guarantees linear-time performance but does not support the full range of Perl-compatible features. Despite these limitations, PCRE2 remains widely used in scripting languages and developer tools due to its flexibility and familiar syntax. \cite{sw_pcre2}

\subsection{Hyperscan}
Hyperscan is Intel’s regular expression matching engine, designed specifically for high-throughput and low-latency applications. It serves as a core component in several security and networking tools, including intrusion detection systems like Suricata and firewalls.

Traditional regex engines (e.g., backtracking-based ones like PCRE) often struggle with performance bottlenecks due to their sequential nature and vulnerability to ReDoS attacks. Hyperscan addresses these limitations by combining multiple automata models—particularly NFAs and DFAs—with a hybrid execution strategy that leverages Single Instruction, Multiple Data (SIMD) parallelism and tiled execution on modern CPUs .

According to Wang et al. (\cite{hyperscan}), Hyperscan divides regexes into multiple subgraphs, such as anchored DFAs for simple patterns and NFAs for complex constructs. This hybrid approach enables it to process large volumes of data streams efficiently without the exponential-time risks associated with backtracking engines.

However, as noted in \cite{hyperscan}, Hyperscan will also enforce syntactic restrictions when compiling. Regexes that are deemed vulnerable or considered ambiguous, therefore limiting expressiveness and versatility, especially when the user is looking for nested repetition (e.g. $(a+)+$, looking to match one or more of one or more $a$'s) or greedy alternation (e.g. $(a|aa)+$, matching a sequence of $a$ or $aa$, repeated, resulting in three matches for a string such as $aa$).

\subsection{Rust's \texttt{regex} library}
Rust is a systems programming language focused on safety, concurrency, and performance, offering memory safety without garbage collection through its unique ownership model. The \texttt{regex} crate implements a hybrid DFA/NFA model and guarantees linear-time performance. All regex searches in Rust's \texttt{regex} library have worst case $O(m * n)$ time complexity, where $m$ is the size of the regular expression and $n$ is the size of the input string. \cite{sw_rustregex}

To uphold this guarantee, the pattern syntax is intentionally restricted: features that are not known to admit efficient matching—most notably \emph{backreferences} and general \emph{look‑around assertions}—are omitted. The engine’s design combines automata techniques: a lazy DFA is used to accelerate finding overall matches, while a PikeVM/Thompson‑NFA simulation recovers submatch boundaries when needed; these strategies avoid recursive backtracking and keep stack usage bounded. The crate provides first‑class Unicode support (including Unicode character classes and word boundaries) and exposes its parser and automata components as reusable libraries (\texttt{regex\_syntax}, \texttt{regex-automata}). In practice, this places \texttt{regex} in the same family as RE2‑style engines: it favors regular‑language constructs and predictable performance over Perl‑style expressiveness.

\section{Tools for ReDoS Detection and Exploitation}
\subsection{Revealer}
REVEALER is a hybrid static–dynamic analysis tool designed to detect and exploit ReDoS vulnerabilities, particularly in regex engines that support extended, non-regular features such as backreferences and lookahead assertions. Unlike traditional ReDoS analyzers that operate under the assumption that regular expressions conform to classical finite automata semantics, REVEALER targets the realities of modern backtracking-based engines like Java’s regex library, where non-regular constructs are common and matching behavior is often implementation-specific \cite{revealer_paper}.
The tool begins with a static analysis phase, modeling regexes as extended nondeterministic finite automata. It detects vulnerable structural patterns (such as nested loops or ambiguous branching) by analyzing the internal state graph used by the Java regex engine. These structures are known to induce exponential or polynomial backtracking behavior under certain input conditions. To confirm whether a suspected pattern can cause denial-of-service, REVEALER proceeds with a dynamic validation phase that simulates matching and attempts to synthesize concrete attack strings. This dual-phase approach minimizes false positives and ensures that reported vulnerabilities are exploitable in practice.
REVEALER was evaluated on over 29,000 Java regexes and successfully identified all previously known vulnerabilities while discovering 213 new ones \cite{revealer_paper}.

%Mention the revealer paper here!
%https://seclab.cse.cuhk.edu.hk/papers/sp21_redos.pdf
%Revealer is a hybrid tool designed to detect and exploit ReDoS vulnerabilities, especially in

\subsection{ReScue}
%ReScue is a technique for detecting ReDoS vulnerabilities in modern regex engines. Unlike traditional static or black-box fuzzing approaches, ReScue leverages the structure of extended nondeterministic finite automata used internally by engines such as Java’s \texttt{java.util.regex}. The tool operates in three stages: a seeding phase that uses a genetic algorithm to generate diverse input strings covering as many e-NFA states as possible; an incubating phase that evolves those seeds toward strings with high cost-effectiveness (i.e., many matching steps per character); and a pumping phase that trims and expands the most promising strings to maximize backtracking behavior. This combination allows ReScue to discover complex vulnerabilities hidden in deep, non-regular constructs such as lookarounds and backreferences.

ReScue is a technique for detecting ReDoS vulnerabilities in modern regex engines. It can generate special input strings that, when submitted as an input to a pre-defined regex engine, will lead to ReDoS. To do this, it starts by exploring the regex engine's behaviors under various inputs, which is called the \textit{seeding phase}. These inputs are selected and evaluated on how many of the e-NFA states they reach, regardless of the time it takes to reach them. After that, it will use the selected strings from the seeding phase and apply "mutations" to them, this phase is called the \textit{incubating phase}. Lastly, the \textit{pumping phase} will enhance the strings by finding which parts of the string take the longest to process, and copy and paste them one after the other. Parallel to this, it will also trim unnecessary parts of the string in order to retain the attack core (the minimal input responsible for the performance hit). \cite{rescue_paper}

ReScue was evaluated on 29088 real-world regexes and outperformed prior tools including RXXR2, Rexploiter, and SlowFuzz, finding 49\% more exploitable inputs and uncovering ten previously unknown vulnerabilities in popular open-source projects \cite{rescue_paper}.

\subsection{ReDoSHunter}
ReDoSHunter is a vulnerability detection framework that combines static analysis with dynamic validation to identify ReDoS vulnerabilities in modern regex engines. Unlike prior approaches that suffer from a trade-off between precision and recall, ReDoSHunter is designed to achieve both high accuracy and coverage.
The framework begins by transforming real-world regexes into a simplified form (standardized regex) while preserving ReDoS-relevant semantics. It then uses pattern-specific static diagnosis algorithms to identify potential vulnerabilities and synthesize attack strings. Each candidate is subsequently validated dynamically against real regex engines to ensure the vulnerability is exploitable in practice. ReDoSHunter also detects multiple vulnerabilities in a single regex, a feature lacking in most prior tools.
Empirically, ReDoSHunter outperforms seven state-of-the-art detection tools in both precision and recall. It achieved 100\% precision and 100\% recall across multiple large-scale datasets containing over 37,000 regexes and discovered 28 previously unknown vulnerabilities in widely used software projects, 26 of which were assigned CVEs \cite{redoshunter_paper}.
This level of precision and recall mean that the tool triggers very few false alarms, and misses very few real vulnerabilities.

\section{Reluctance}
Theory for automata has been around for very long, and its theory has been checked and developed by many. However, software is often built from demand. Most of the time, someone builds something because it could be useful. Regular expressions entered practical programming through Ken Thompson's implementation in the QED text editor on CTSS (and later, in Dennis Ritchie's version for GE-TSS). When Thompson and Ritchie developed Unix, they carried regular expressions with them to the new ecosystem, adding them to tools such as \texttt{awk} and \texttt{sed}.
By the end of the 1970s, regular expressions were a fundamental part of Unix. Nowadays, regular expressions illustrate the risks of straying from theoretical principles. Contemporary regex engines (like backtracking, for example) are often far less efficient than the real automata-based implementations used in the early Unix tools.
Despite known limitations and security risks associated with traditional backtracking-based regular expression engines, there remains significant resistance to replacing them in many modern and legacy systems. This reluctance doesn't stem only from the fact that people don't want to change stuff if it "works", especially when it is so embedded into a trustworthy system:
\begin{itemize}
	\item \textbf{Backwards compatibility}: Changing old code could break countless projects, many of them supporting running projects today
	\item \textbf{Pattern rewritting}: While tools like RE2 offer guaranteed linear-time matching, they lack features such as backreferences or lookaround assertions, which make them unsuitable for existing expressions that depend on those features
	\item \textbf{The risk}: Many organizations fear regressions or downtime, and often ponder if changing this old code is worth it
\end{itemize}

For instance, Irani et. al (\cite{reluctance_study}) performed a study within the public administration domain but it can be applied here. They found that there are multiple layers of complexity that prevent modernization, ranging from outdated architectures to deep interdependencies across institutions. The systems they studied were often tied to statutory processes, policy constraints and siloed governance, making them difficult to refactor or retire.
Irani et. al found that overcoming a legacy system's inertia to change often requires more than just technical solutions. It demands stakeholder buy-in, incremental migration strategies, semantic data interoperability and, most importantly, alignment between policy ambitions and organizational capacity.











