\chapter{Introduction}\label{chap:intro}

%https://www.usenix.org/system/files/sec22-turonova.pdf
%https://arxiv.org/abs/2407.20479
%https://www.regular-expressions.info/catastrophic.html

In this chapter, the problem is overviewed, the study's importance is explained along with goals for the proposed solution. 

\section{Background}
Regular expressions are a foundational tool in computer science, widely used in pattern matching, lexical analysis, input validation, and string processing. Their expressiveness and concise syntax make them a powerful language for describing regular languages.

% However, when implemented without care, especially in backtracking-based engines, regular expressions can become a source of serious security vulnerabilities.

A regular expression $R$ is used (along with an input $W$) in regex matching engines. The matching engines will verify if $W$ is fully matched by $R$, meaning that the entire input is a match - or they will verify if a substring of $W$ is matched by $R$. 

% Current matching engines commonly fall into one of the following categories:
% \begin{itemize}
% 	\item \textbf{Finite State Machine Regular Expression Engines}: A finite state machine (or finite \emph{automaton}) is built and evaluated using every symbol $\sigma \in W$. Used \emph{UNIX}-based systems.
% 	\item \textbf{Backtracking Regular Expression Engines}: Instead of building a finite state machine, 
% \end{itemize}

\section{Regular Expression Denial of Service}
One such vulnerability is known as \emph{Regular Expression Denial of Service} (ReDoS). ReDoS exploits the pathological worst-case behavior of certain regular expressions, causing exponential time complexity during matching. In typical backtracking matchers---such as those found in JavaScript, Java, and many scripting environments---ambiguous or nested expressions (especially involving repetition, such as \texttt{(a+)+}) can lead the engine to explore an exponential number of paths for certain crafted inputs. This behavior allows an attacker to intentionally supply inputs that force excessive computation, effectively rendering a service unavailable or degraded.

The root of the ReDoS problem lies not in regular expressions as a theoretical model but in how they are operationalized in software. While deterministic finite automata (DFAs) evaluate regular expressions in linear time, many real-world engines opt for backtracking NFAs due to their flexibility and ease of implementation. Unfortunately, these NFAs are susceptible to exponential blow-up in ambiguous or unguarded patterns.

\section{Case Studies}
In this section, two case studies are presented. These serve as a form of introduction to getting to know and understand the ReDoS problem.

\subsection{Stack Overflow}
\label{intro:case_studies:stack_overflow}
On the 20th of July 2016, a user published an malformed post on the online information exchange forum \textit{Stack Overflow}. A couple minutes after the post, at around 14:44 UTC, the entire website became unavailable for around 34 minutes, after which there was an update that fixed the underlying issue.

On the forum, there is an automatic text formatter that runs every time someone posts something. This tool will trim unicode spaces from the start of a line until its end. The regular expression responsible for matching these spaces was the following:

\begin{center}
	$\textasciicircum [ \textbackslash s \textbackslash u200c]+|[ \textbackslash s \textbackslash u200c]+\$$
\end{center}

\begin{itemize}
	\item \textbf{$\textasciicircum$} will anchor the following expression to the start of a string
	\item \textbf{$[ \textbackslash s \textbackslash u200c]+$} will match either one whitespace character (space, tab, newline, etc..) or the unicode character U+200C, also known as the \textit{Zero Width Non-Joiner}
	\textit \textbf{$|$} is the union operator, meaning that the expression will match either the left or right side expressions (relative to this operator)
	\item \textbf{$\$$} will anchor the match to the end of that string
\end{itemize}


The automatic text formatter contains a matcher that tried to match the regular expression described above against an input that contained around 20,000 consecutive whitespace characters on one line that started with $--$.
The backtracking matcher in place worked as follows:

Given an input string $M$ of length $\text{len}(M)$, let $n_k$ denote the character at position $k$, where $0 \leq k < \text{len}(M)$. For each possible starting position $p$ such that $0 \leq p < \text{len}(M)$, perform the following steps:

\begin{enumerate}
	\item Check whether the character $n_k$ (for $k \geq p$) belongs to either the character class \verb|\s| (whitespace) or the Unicode character \verb|\u200c| (zero-width non-joiner).
	\item Continue checking characters until a character not in the above classes is encountered or the end of the string is reached.
	\item If the end of the string is reached and all characters from position $p$ onward matched, the pattern succeeds.
	\item If a non-matching character is encountered before the end of the string, the match fails at this position; increment $p$ and repeat from step 1.
\end{enumerate}

For a $20{,}000$ whitespace-character input, the sum of computations is given as follows:

\begin{center}
	\[ \sum_{k=1}^{20,000} k = \frac{20,000 \cdot (20,000) + 1}{2} = 200,010,000 \]
\end{center}

This means that this matching algorithm ran in $O(n^2)$ complexity, and this blow up was to be expected.

The engineers quickly fixed this issue by switching to a substring replacing method.

\subsection{minimatch}
\textbf{minimatch} is a minimal matching utility, used internally by the \textbf{Node Package Manager}, better known as \textbf{npm}. \cite{npm_minimatch}
This utility works by convering glob expressions into JavaScript's \textit{RegExp} objects, supporting the following glob features:
\begin{itemize}
	\item Brace Expansion
	\item Extended glob matching
	\item "Globstar" (**) matching
	\item POSIX character classes
\end{itemize}
The utility has millions of downloads, as it is an essential component of \textbf{npm}.

On the 18th of October, a new CVE was introduced: \textbf{CVE-2022-3517}.
The report showed that all of minimatch's versions below 3.0.5 were vulnerable to a ReDoS attack similar to the one described in \ref{intro:case_studies:stack_overflow}.
The culprit for this was a function called \textit{braceExpand}, which is responsible for expanding brace patterns in glob strings. This is commonly known as brace expansion and is often seen in Unix shells (such as bash). 
The function contained a regular expression that would match against given patterns and decide if a brace expansion was in order. The expression used was "/\textbackslash\{.*\textbackslash\}/)", which matches any string containing a single pair of curly braces with any characters inside. But this expression poses an issue:

For example, the following text:
\begin{center}
	"\{\{\{\{\{\{\{\{\{\{\{\{\{...X"
\end{center}

with the \{ repeated over 30,000 times and no closing \}, can cause a significant CPU spike or even hang the process due to catastrophic backtracking.

To fix this issue, the developer decided to switch to a safer regular expression: \textbackslash\{(?:(?!\textbackslash\{).)*\textbackslash\}/


\section{Reluctance Toward Changing Legacy Matchers}

Despite well-documented vulnerabilities such as ReDoS, there remains significant reluctance in the software engineering community to replace or refactor legacy regex engines—particularly those built into performance-critical or widely adopted tools such as \texttt{grep}, \texttt{sed}, and many scripting languages.

These tools often rely on matching engines that prioritize speed and simplicity of implementation over safety. For example, \texttt{grep} and similar UNIX utilities implement regex matchers using finite automata, but their behavior with extended features (like bounded repetitions) can still lead to performance degradation in edge cases. While these engines are generally immune to the exponential blow-up typical of backtracking matchers, they may suffer from linear but high-cost processing when automata grow excessively large due to poorly constructed patterns.

The situation is more severe in environments that rely on backtracking matchers, such as JavaScript, Java, and many shell-based text processors. In these ecosystems, regular expressions are both expressive and dangerously permissive, allowing patterns that trigger catastrophic backtracking without warning.

Refactoring or replacing these engines is often resisted for several reasons:
\begin{itemize}
	\item \textbf{Backwards compatibility}: Legacy codebases and systems expect specific regex semantics, and changing the underlying engine could break existing behavior.
	\item \textbf{Perceived performance cost}: DFA-based matchers may require significant memory or preprocessing, which is viewed as a performance risk in lightweight tools.
	\item \textbf{Lack of awareness}: Many developers are unaware that regex matchers can introduce denial-of-service vulnerabilities, especially when ReDoS exploits are subtle or input-driven.
	\item \textbf{Cultural inertia}: Tools like \texttt{grep} are deeply embedded in developer workflows and scripts, making any modification to their behavior or performance profile controversial.
\end{itemize}

Even modern matchers that are designed to avoid ReDoS—such as Google’s RE2 or Rust’s regex crate—are often underutilized due to these legacy constraints.

These factors highlight the need for not only technical solutions, such as safer regex engines and static analysis tools, but also a cultural shift in how regular expressions are authored, reviewed, and validated in production systems.

The next chapter will give some more insight into the basics of regular expressions and automata.

%Giv
%
%For a character $n_k$ in the input text $M$ whose position is given by $0 \le k < \text{len(M)}$, start position $0 \le p < \text{len(M)}$.
%\begin{enumerate}
%	\item Check if $n_k$ belongs to either the $\textbackslash s$ or $\textbackslash u200c$ character class
%	\item If $n_k$ doesn't belong, 
%\end{enumerate}

%\begin{enumerate}
%	\item The regex engine attempts to match the expression \verb|^[\s\u200c]+| or \verb|[\s\u200c]+$| against an input string. This pattern is used to trim leading or trailing whitespace and zero-width non-joiner (ZWNJ) characters.
%	
%	\item The second part of the regex \verb|[\s\u200c]+$| tries to match one or more trailing whitespace/ZWNJ characters up to the end of the string.
%	
%	\item The backtracking engine begins from the first space and tries to consume all 20{,}000 whitespace characters, expecting the end of the string immediately after.
%	
%	\item When it finds `--` instead of end-of-string, the match fails, and the engine backtracks:
%	\begin{enumerate}
%		\item It retries the match starting from the second space, then the third, and so on.
%		\item Each attempt involves checking whether the substring from that point matches the pattern ending in \verb|$|.
%	\end{enumerate}
%	
%	\item This leads to a total of:
%	\[
%	\sum_{i=1}^{20{,}000} i = \frac{20{,}000 \cdot 20{,}001}{2} = 200,\!010,\!000
%	\]
%	character class checks, resulting in $O(n^2)$ time complexity.
%	
%	\item This CPU-intensive behavior caused the web server to become unresponsive, triggering a system-wide outage due to the homepage failing health checks.
%	
%	\item The issue was mitigated by replacing the regex with a faster substring-based trimming function.
%\end{enumerate}

%The backtracking matcher begins scanning from the first character of the input. For each position $i$, it attempts to match the entire suffix using the pattern (e.g., $\s+$).
%If the match fails at the end, it backtracks and tries from the next character, leading to $O(n^2)$ complexity when the input consists of long repeated whitespace sequences. This behavior caused severe CPU spikes when the 20,000-space input was processed, triggering a denial-of-service condition.




