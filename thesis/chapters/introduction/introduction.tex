\chapter{Introduction}\label{chap:intro}

%https://www.usenix.org/system/files/sec22-turonova.pdf
%https://arxiv.org/abs/2407.20479
%https://www.regular-expressions.info/catastrophic.html

Regular expressions are a powerful and ubiquitous tool in software development, enabling concise and expressive definitions of complex text patterns. However, their utility comes with a hidden cost: when implemented naively or misused, certain regular expression constructs can expose systems to catastrophic performance degradation. This vulnerability, known as Regular Expression Denial of Service (ReDoS), arises particularly in backtracking-based matching engines and has been responsible for real-world outages across major platforms. 

This chapter introduces the ReDoS problem by first providing foundational background on regular expression engines and their operational behavior. It then outlines the nature of ReDoS vulnerabilities, illustrating their practical impact through two real-world case studies. Finally, it discusses the reasons for continued reliance on legacy engines despite known risks. By establishing the technical and practical motivations behind this research, this chapter sets the stage for the proposed approach introduced in the subsequent chapters.

\section{Background}
Regular expressions are a foundational tool in computer science, widely used in pattern matching, lexical analysis, input validation, and string processing. Their expressiveness and concise syntax make them a powerful language for describing regular languages.

% However, when implemented without care, especially in backtracking-based engines, regular expressions can become a source of serious security vulnerabilities.

A regular expression $R$ is used (along with an input $W$) in regex matching engines. The matching engines will verify if $W$ is fully matched by $R$, meaning that the entire input is a match - or they will verify if a substring of $W$ is matched by $R$. 

% Current matching engines commonly fall into one of the following categories:
% \begin{itemize}
% 	\item \textbf{Finite State Machine Regular Expression Engines}: A finite state machine (or finite \emph{automaton}) is built and evaluated using every symbol $\sigma \in W$. Used \emph{UNIX}-based systems.
% 	\item \textbf{Backtracking Regular Expression Engines}: Instead of building a finite state machine, 
% \end{itemize}

\section{Regular Expression Denial of Service}
One such vulnerability is known as \emph{Regular Expression Denial of Service} (ReDoS). ReDoS exploits the pathological worst-case behavior of certain regular expressions, causing exponential running time complexity during matching.
% NAO ME LEMBRO DO QUE METER AQUI

In commonly used backtracking matchers (such as those found in JavaScript, Java, and many scripting environments) ambiguous or nested expressions (such as '\texttt{(a+)+}', which involves repetition) can lead the engine to explore an exponential number of paths for certain crafted inputs. This behavior allows an attacker to intentionally supply inputs that force excessive computation, effectively rendering a service unavailable or degraded.

The root of the ReDoS problem lies not in regular expressions as a theoretical model, but in how they are implemented in many real-world software systems—particularly through backtracking-based matchers. While deterministic finite automata (DFAs) evaluate regular expressions in linear time, backtracking matchers explore multiple paths through the input, making them susceptible to exponential blow-up in the presence of ambiguous or nested patterns.

\section{Case Studies}
In this section, two case studies are presented. These serve as a form of introduction to getting to know and understand the ReDoS problem.

\subsection{Stack Overflow}
\label{intro:case_studies:stack_overflow}
On the 20th of July 2016, a user published a malformed post on the online information exchange forum \textit{Stack Overflow}. A couple minutes after the post, at around 14:44 UTC, the entire website became unavailable for around 34 minutes, after which there was an update that fixed the underlying issue.

On the forum, there is an automatic text formatter that runs every time someone posts something. This tool will trim any group of leading whitespace or invisible characters at both the beginning and end of a post. The regular expression that does so is the following:

\begin{center}
	$\textasciicircum [ \textbackslash s \textbackslash u200c]+|[ \textbackslash s \textbackslash u200c]+\$$
\end{center}

\begin{itemize}
	\item \textbf{$\textasciicircum$} will anchor the following expression to the start of a string
	\item \textbf{$[ \textbackslash s \textbackslash u200c]+$} will match one or more of either whitespace characters (space, tab, newline, etc..) or the unicode characters U+200C (zero-width non-joiner)
	\item \textbf{$\$$} will anchor the match to the end of that string
\end{itemize}


The aforementioned tool was an automatic text formatter which contained a matcher that tried to match the regular expression described above against an input that contained around 20,000 consecutive whitespace characters on one line that started with $--$.
The backtracking matcher in place worked as follows:

Given an input string $M$ of length $\text{len}(M)$, let $n_k$ denote the character at position $k$, where $0 \leq k < \text{len}(M)$. For each possible starting position $p$ such that $0 \leq p < \text{len}(M)$, perform the following steps:

\begin{enumerate}
	\item Check whether the character $n_k$ (for $k \geq p$) is either a whitespace or a zero-width non-joiner (U+200C).
	\item Continue checking characters until it reaches a character that is neither a whitespace nor a zero-width non-joiner.
	\item If the end of the string is reached and all characters from position $p$ onward matched, the pattern succeeds.
	\item If a non-matching character is encountered before the end of the string, the match fails at this position; increment $p$ and repeat from step 1.
\end{enumerate}

For a $20{,}000$ whitespace-character (both whitespace and zero-width non-joiner characters) input, the sum of computations is given as follows:

\begin{center}
	\[ \sum_{k=1}^{20,000} k = \frac{20,000 \cdot (20,000) + 1}{2} = 200,010,000 \]
\end{center}

This means that the matching algorithm ran in $O(n^2)$ complexity, and this blow up was to be expected.

The engineers quickly fixed this issue by switching to a substring replacing method.

\subsection{minimatch}
\textbf{minimatch} is a minimal matching utility, used internally by the \textbf{Node Package Manager}, better known as \textbf{npm}. \cite{npm_minimatch}
This utility works by converting glob expressions into JavaScript's \textit{RegExp} objects, supporting the following glob features:
\begin{itemize}
	\item Brace Expansion
	\item Extended glob matching
	\item "Globstar" (**) matching
	\item POSIX character classes
\end{itemize}
The utility has millions of downloads, as it is an essential component of \textbf{npm}.

On the 18th of October 2022, a new CVE was introduced: \textbf{CVE-2022-3517}.
The report showed that all of minimatch's versions below 3.0.5 were vulnerable to a ReDoS attack similar to the one described in Subsection~\ref{intro:case_studies:stack_overflow}.
The culprit for this was a function called \textit{braceExpand}, which is responsible for expanding brace patterns in glob strings. This is commonly known as brace expansion and is often seen in Unix shells (such as bash). 
The function contained a regular expression that would match against given patterns and decide if a brace expansion was in order. The expression used was "/\textbackslash\{.*\textbackslash\}/", which matches any string containing a single pair of curly braces with any characters inside. But this expression poses an issue:

For example, the following text:
\begin{center}
	"\{\{\{\{\{\{\{\{\{\{\{\{\{...X"
\end{center}

with the opening brace \texttt{\{} repeated over 30,000 times and no closing brace \texttt{\}}, can cause a significant CPU spike or even hang the process due to catastrophic backtracking.

To fix this issue, the developer decided to switch to a safer regular expression: \textbackslash\{(?:(?!\textbackslash\{).)*\textbackslash\}/

This new pattern prevents the exponential blow-up because it removes the ambiguity introduced by the symbol \texttt{.}, which is the greedy quantifier. In the original expression, the greedy quantifier could match arbitrarily long substrings and repeatedly backtrack in search of a closing brace symbol, resulting in exponential-time behavior when no match was found.

In contrast, the new expression uses a \textit{negative lookahead}, which ensures that each symbol is consumed only if it is not followed by another opening brace, thereby eliminating redundant backtracking.

\section{Reluctance Toward Changing Legacy Matchers}

Despite well-documented vulnerabilities such as ReDoS, there remains significant reluctance in the software engineering community to replace or refactor legacy regex engines—particularly those built into performance-critical or widely adopted tools such as \texttt{grep}, \texttt{sed}, and many scripting languages.

These tools often rely on matching engines that prioritize speed and simplicity of implementation over safety. For example, \texttt{grep} and similar UNIX utilities implement regex matchers using finite automata, but their behavior with extended features (like bounded repetitions) can still lead to performance degradation in edge cases. While these engines are generally immune to the exponential blow-up typical of backtracking matchers, they may suffer from linear but high-cost processing when automata grow excessively large due to poorly constructed patterns.

The situation is more severe in environments that rely on backtracking matchers, such as JavaScript, Java, and many shell-based text processors. In these ecosystems, regular expressions are both expressive and dangerously permissive, allowing patterns that trigger catastrophic backtracking without warning.

Refactoring or replacing these engines is often resisted for several reasons:
\begin{itemize}
	\item \textbf{Backwards compatibility}: Legacy codebases and systems expect specific regex semantics, and changing the underlying engine could break existing behavior.
	\item \textbf{Perceived performance cost}: DFA-based matchers may require significant memory or preprocessing, which is viewed as a performance risk in lightweight tools.
	\item \textbf{Lack of awareness}: Many developers are unaware that regex matchers can introduce denial-of-service vulnerabilities, especially when ReDoS exploits are subtle or input-driven.
	\item \textbf{Cultural inertia}: Tools like \texttt{grep} are deeply embedded in developer workflows and scripts, making any modification to their behavior or performance profile controversial.
\end{itemize}

Even modern matchers that are designed to avoid ReDoS—such as Google’s RE2 or Rust’s regex crate—are often underutilized due to these legacy constraints.

These factors highlight the need for not only technical solutions, such as safer regex engines and static analysis tools, but also a cultural shift in how regular expressions are authored, reviewed, and validated in production systems.

The next chapter will give some more insight into the basics of regular expressions and automata.

%Giv
%
%For a character $n_k$ in the input text $M$ whose position is given by $0 \le k < \text{len(M)}$, start position $0 \le p < \text{len(M)}$.
%\begin{enumerate}
%	\item Check if $n_k$ belongs to either the $\textbackslash s$ or $\textbackslash u200c$ character class
%	\item If $n_k$ doesn't belong, 
%\end{enumerate}

%\begin{enumerate}
%	\item The regex engine attempts to match the expression \verb|^[\s\u200c]+| or \verb|[\s\u200c]+$| against an input string. This pattern is used to trim leading or trailing whitespace and zero-width non-joiner (ZWNJ) characters.
%	
%	\item The second part of the regex \verb|[\s\u200c]+$| tries to match one or more trailing whitespace/ZWNJ characters up to the end of the string.
%	
%	\item The backtracking engine begins from the first space and tries to consume all 20{,}000 whitespace characters, expecting the end of the string immediately after.
%	
%	\item When it finds `--` instead of end-of-string, the match fails, and the engine backtracks:
%	\begin{enumerate}
%		\item It retries the match starting from the second space, then the third, and so on.
%		\item Each attempt involves checking whether the substring from that point matches the pattern ending in \verb|$|.
%	\end{enumerate}
%	
%	\item This leads to a total of:
%	\[
%	\sum_{i=1}^{20{,}000} i = \frac{20{,}000 \cdot 20{,}001}{2} = 200,\!010,\!000
%	\]
%	character class checks, resulting in $O(n^2)$ time complexity.
%	
%	\item This CPU-intensive behavior caused the web server to become unresponsive, triggering a system-wide outage due to the homepage failing health checks.
%	
%	\item The issue was mitigated by replacing the regex with a faster substring-based trimming function.
%\end{enumerate}

%The backtracking matcher begins scanning from the first character of the input. For each position $i$, it attempts to match the entire suffix using the pattern (e.g., $\s+$).
%If the match fails at the end, it backtracks and tries from the next character, leading to $O(n^2)$ complexity when the input consists of long repeated whitespace sequences. This behavior caused severe CPU spikes when the 20,000-space input was processed, triggering a denial-of-service condition.




