% This file was created with JabRef 2.10.
% Encoding: Cp1252

@manual{sw_rustregex,
	title  = {regex — Rust crate documentation},
	note   = {\url{https://docs.rs/regex/latest/regex/}},
	year   = {2025}
}

@software{sw_pcre2,
	author = {University of Cambridge},
	title = {pcre2},
	url = {https://pcre.org/current/doc/html/pcre2.html},
	date = {2024-12-18},
	version = {10.45}
}

@InProceedings{campeanu_reg_ereg,
	author="C{\^a}mpeanu, Cezar
	and Salomaa, Kai
	and Yu, Sheng",
	editor="Champarnaud, Jean-Marc
	and Maurel, Denis",
	title="Regex and Extended Regex",
	booktitle="Implementation and Application of Automata",
	year="2003",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="77--84",
	abstract="Regex are used in many programs such as Perl, Awk, Python, egrep, vi, emacs etc. It is known that regex are different from regular expressions. In this paper, we give regex a formal treatment. We make a distinction between regex and extended regex; while regex present regular languages, extended regex present a family of languages larger than regular languages. We prove a pumping lemma for the languages expressed by extended regex. We show that the languages represented by extended regex are incomparable with context-free languages and a proper subset of context-sensitive languages.",
	isbn="978-3-540-44977-5"
}

@software{russ_cox_re2,
	author = {Google},
	title = {RE2, a regular expression library},
	url = {https://github.com/google/re2},
}

@online{russ_cox_regexp3,
	title = {Regular Expression Matching in the Wild},
	author = {Cox, Russ},
	year = 2010,
	url = {https://swtch.com/~rsc/regexp/regexp3.html},
	urldate = {March 2010}
}

@software{sw_grep,
	author = {{Free Software Foundation}},
	title = {grep},
	url = {https://www.gnu.org/software/grep/},
	version = {3.12},
	date = {2025-04-23}
}

@software{sw_sed,
	author = {{Free Software Foundation}},
	title = {sed},
	url = {https://www.gnu.org/software/sed/},
	version = {4.9}
}

@manual{sw_gawk,
	title     = {GNU Awk User's Guide},
	organization = {Free Software Foundation},
	url      = {https://www.gnu.org/software/gawk/manual/gawk.html},
	year      = {2024}
}

@manual{regex_opengroup,
	title = {The Open  Group Base Specifications Issue 8},
	organization = {The IEEE and The Open Group},
	year = {2024},
	url = {https://pubs.opengroup.org/onlinepubs/9799919799/}
	
}

@article{brzozowski_derivatives,
	author = {Brzozowski, Janusz A.},
	title = {Derivatives of Regular Expressions},
	year = {1964},
	issue_date = {Oct. 1964},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {11},
	number = {4},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/321239.321249},
	doi = {10.1145/321239.321249},
	journal = {J. ACM},
	month = oct,
	pages = {481–494},
	numpages = {14}
}

@article{mesh-of-automata,
	title = {{A mesh of automata}},
	author = {S Broda and M Holzer and E Maia and N Moreira and R Reis},
	doi = {10.1016/j.ic.2019.01.003},
	journal = {Information and Computation},
	pages = {94-111},
	publisher = {Academic Press Inc Elsevier Science},
	type = {Article},
	volume = {265},
	year = {2019}
}

% Replace with first edition
@article{intro-to-automata-languages-computation,
  author = {Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D.},
  title = {Introduction to automata theory, languages, and computation, 2nd edition},
  year = {2001},
  issue_date = {March 2001},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {32},
  number = {1},
  issn = {0163-5700},
  url = {https://doi.org/10.1145/568438.568455},
  doi = {10.1145/568438.568455},
  journal = {SIGACT News},
  month = mar,
  pages = {60-65},
  numpages = {6}
}

@article{fado_paper,
  author = {Reis, Rogério and Moreira, Nelma},
  year = {2002},
  month = {11},
  pages = {},
  title = {FAdo: tools for finite automata and regular expressions manipulation}
}

@online{npm_minimatch,
  author = {Z. Schlueter, Isaac},
  year = 2011,
  title = {minimatch},
  url = {https://www.npmjs.com/package/minimatch},
  urldate = {2025-07-21}
}

@inproceedings {hyperscan,
	author = {Xiang Wang and Yang Hong and Harry Chang and KyoungSoo Park and Geoff Langdale and Jiayu Hu and Heqing Zhu},
	title = {Hyperscan: A Fast Multi-pattern Regex Matcher for Modern {CPUs}},
	booktitle = {16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)},
	year = {2019},
	isbn = {978-1-931971-49-2},
	address = {Boston, MA},
	pages = {631--648},
	url = {https://www.usenix.org/conference/nsdi19/presentation/wang-xiang},
	publisher = {USENIX Association},
	month = feb
}

@article{pdregex_antimirov,
	title = {Partial derivatives of regular expressions and finite automaton constructions},
	journal = {Theoretical Computer Science},
	volume = {155},
	number = {2},
	pages = {291-319},
	year = {1996},
	issn = {0304-3975},
	doi = {https://doi.org/10.1016/0304-3975(95)00182-4},
	url = {https://www.sciencedirect.com/science/article/pii/0304397595001824},
	author = {Valentin Antimirov},
	abstract = {We introduce a notion of partial derivative of a regular expression and apply it to finite automaton constructions. The notion is a generalization of the known notion of word derivative due to Brzozowski: partial derivatives are related to non-deterministic finite automata (NFA's) in the same natural way as derivatives are related to deterministic ones (DFA's). We give a constructive definition of partial derivatives and prove several facts, in particular: 1.(1) any derivative of a regular expression r can be represented by a finite set of partial derivatives of r;2.(2) the set of all partial derivatives of r is finite and its cardinality is less than or equal to one plus the number of occurrences of letters from A appearing in r;3.(3) any partial derivative of r is either a regular unit, or a subterm of r, or a concatenation of several such subterms. These theoretical results lead us to a new algorithm for turning regular expressions into relatively small NFA's and allow us to provide certain improvements to Brzozowski's algorithm for constructing DFA's. We also report on a prototype implementation of our NFA construction and present several examples.}
}

@article{thompson_re2nfa,
	author = {Thompson, Ken},
	title = {Programming Techniques: Regular expression search algorithm},
	year = {1968},
	issue_date = {June 1968},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {11},
	number = {6},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/363347.363387},
	doi = {10.1145/363347.363387},
	abstract = {A method for locating specific character strings embedded in character text is described and an implementation of this method in the form of a compiler is discussed. The compiler accepts a regular expression as source language and produces an IBM 7094 program as object language. The object program then accepts the text to be searched as input and produces a signal every time an embedded string in the text matches the given regular expression. Examples, problems, and solutions are also presented.},
	journal = {Commun. ACM},
	month = jun,
	pages = {419–422},
	numpages = {4},
	keywords = {match, regular expression, search}
}

@article{resharp_tool_paper,
	title={RE\#: High Performance Derivative-Based Regex Matching with Intersection, Complement, and Restricted Lookarounds},
	volume={9},
	ISSN={2475-1421},
	url={http://dx.doi.org/10.1145/3704837},
	DOI={10.1145/3704837},
	number={POPL},
	journal={Proceedings of the ACM on Programming Languages},
	publisher={Association for Computing Machinery (ACM)},
	author={Varatalu, Ian Erik and Veanes, Margus and Ernits, Juhan},
	year={2025},
	month=jan, pages={1–32}
}

@inproceedings {hyperscan_paper,
	author = {Xiang Wang and Yang Hong and Harry Chang and KyoungSoo Park and Geoff Langdale and Jiayu Hu and Heqing Zhu},
	title = {Hyperscan: A Fast Multi-pattern Regex Matcher for Modern {CPUs}},
	booktitle = {16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)},
	year = {2019},
	isbn = {978-1-931971-49-2},
	address = {Boston, MA},
	pages = {631--648},
	url = {https://www.usenix.org/conference/nsdi19/presentation/wang-xiang},
	publisher = {USENIX Association},
	month = feb
}

@INPROCEEDINGS{revealer_paper,
	author={Liu, Yinxi and Zhang, Mingxue and Meng, Wei},
	booktitle={2021 IEEE Symposium on Security and Privacy (SP)}, 
	title={Revealer: Detecting and Exploiting Regular Expression Denial-of-Service Vulnerabilities}, 
	year={2021},
	volume={},
	number={},
	pages={1468-1484},
	keywords={Privacy;Java;Computer languages;Prototypes;Static analysis;Tools;Feature extraction},
	doi={10.1109/SP40001.2021.00062}
}

@inproceedings{rescue_paper,
	author = {Shen, Yuju and Jiang, Yanyan and Xu, Chang and Yu, Ping and Ma, Xiaoxing and Lu, Jian},
	title = {ReScue: crafting regular expression DoS attacks},
	year = {2018},
	isbn = {9781450359375},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3238147.3238159},
	doi = {10.1145/3238147.3238159},
	abstract = {Regular expression (regex) with modern extensions is one of the most popular string processing tools. However, poorly-designed regexes can yield exponentially many matching steps, and lead to regex Denial-of-Service (ReDoS) attacks under well-conceived string inputs. This paper presents Rescue, a three-phase gray-box analytical technique, to automatically generate ReDoS strings to highlight vulnerabilities of given regexes. Rescue systematically seeds (by a genetic search), incubates (by another genetic search), and finally pumps (by a regex-dedicated algorithm) for generating strings with maximized search time. We implemenmted the Rescue tool and evaluated it against 29,088 practical regexes in real-world projects. The evaluation results show that Rescue found 49\% more attack strings compared with the best existing technique, and applying Rescue to popular GitHub projects discovered ten previously unknown ReDoS vulnerabilities.},
	booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
	pages = {225–235},
	numpages = {11},
	keywords = {ReDoS, denial of service, egular expression, genetic algorithm},
	location = {Montpellier, France},
	series = {ASE '18}
}

@inproceedings {redoshunter_paper,
	author = {Yeting Li and Zixuan Chen and Jialun Cao and Zhiwu Xu and Qiancheng Peng and Haiming Chen and Liyuan Chen and Shing-Chi Cheung},
	title = {{ReDoSHunter}: A Combined Static and Dynamic Approach for Regular Expression {DoS} Detection},
	booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
	year = {2021},
	isbn = {978-1-939133-24-3},
	pages = {3847--3864},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/li-yeting},
	publisher = {USENIX Association},
	month = aug
}

@inproceedings{turning_evil_regexps_harmless,
	author = {van der Merwe, Brink and Weideman, Nicolaas and Berglund, Martin},
	title = {Turning evil regexes harmless},
	year = {2017},
	isbn = {9781450352505},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3129416.3129440},
	doi = {10.1145/3129416.3129440},
	abstract = {We explore the relationship between ambiguity in automata and regular expressions on the one hand, and the matching time of backtracking regular expression matchers on the other. We focus in particular on the extreme cases where we have either an exponential amount of ambiguity or no ambiguity at all. We also investigate techniques to reduce or remove ambiguity from regular expressions, which can then be used to transform regular expressions which might be exploited by using algorithmic complexity, into harmless equivalent expressions.},
	booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists},
	articleno = {38},
	numpages = {10},
	keywords = {ReDoS, algorithmic complexity attack, ambiguity, backtracking matcher, regular expression},
	location = {Thaba 'Nchu, South Africa},
	series = {SAICSIT '17}
}

@article{reluctance_study,
	title = {The impact of legacy systems on digital transformation in European public administration: Lesson learned from a multi case analysis},
	journal = {Government Information Quarterly},
	volume = {40},
	number = {1},
	pages = {101784},
	year = {2023},
	issn = {0740-624X},
	doi = {https://doi.org/10.1016/j.giq.2022.101784},
	url = {https://www.sciencedirect.com/science/article/pii/S0740624X22001204},
	author = {Zahir Irani and Raul M. Abril and Vishanth Weerakkody and Amizan Omar and Uthayasankar Sivarajah},
	keywords = {Legacy system, Digital transformation, Public administration, E-government, European, Public sector},
	abstract = {Legacy systems have continued to pose a major challenge to digital transformation efforts in public administration. A comprehensive review of literature suggests seven levels of complexity in transforming legacy systems, including, being a stand-alone system, being part of a larger system, and data incompatibility, each depicting unique criteria and challenges. Nonetheless, very little is known as to what degree these complexities implicate the implementation of digital transformation efforts in public administration (PA). To address this gap, this research conducted an analysis on four cases of digital transformation in three European PA settings (i.e., Denmark, the Netherlands, and the UK). The findings revealed complexities that pose the key challenges to systems interoperability and integrability, which are crucial in any digital transformation project. In addition, a comprehensive understanding of the systems to be transformed, the policies which they are serving, and the broader PA setting in which they are implemented were deemed central to succeeding in digital transformation efforts.}
}

@misc{lark_parser,
	author       = {Erez Shinan},
	title        = {Lark — a parsing toolkit for Python},
	year         = {2017},
	url          = {https://github.com/lark-parser/lark},
	note         = {Version 1.x, Available at: \url{https://github.com/lark-parser/lark}},
}
